{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphSSL",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memazouni/A-Comprehensive-ML-Workflow-for-HousePrices/blob/master/GraphSSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOYNxSMk-0gY"
      },
      "source": [
        "# **Self-Supervised Learning for Graphs**\n",
        "This colab serves as a tutorial on using self-supervised learning for graphs. Self-supervised learning is a class of unsupervised machine learning methods where the goal is to learn rich representations of unstructured data when we do not have access to any labels. This repository implements a variety of commonly used methods (augmentations, encoders, loss functions) for self-supervised learning on graphs. The codebase also includes the option of loading commonly used graph datasets for a variety of downstream tasks. It is built using [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) which is a library built on PyTorch for graph machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBkuRxg4Li4"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cells below ensure the correct installation of torch-geometric and clone the repository which has extra utitlity code which is required to train the models. The entire code repository for GraphSSL can be found on [Github](https://github.com/paridhimaheshwari2708/GraphSSL.git)."
      ],
      "metadata": {
        "id": "RlVYNxXt1Gcb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE4ypnCgG42u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24536e4b-ac17-49a4-bf96-72d3057fcfb6"
      },
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric\n",
        "!git clone https://github.com/paridhimaheshwari2708/GraphSSL.git\n",
        "%cd /content/GraphSSL/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=e042e03cb7d8e29aede553ff5eda64a8fff3375510f9692a8ab2604391d77661\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n",
            "Cloning into 'GraphSSL'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 187 (delta 110), reused 111 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (187/187), 55.18 KiB | 13.79 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "/content/GraphSSL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjHI8t0h3ydt"
      },
      "source": [
        "## Setting up arguments to train the self-supervised model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcj1YQ7Z0vbB"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "Change these arguments to change either the dataset / model / loss function / types of augmentations.\n",
        "The augmentations mentioned in augment_list shall be applied sequentially to generate a positive pair for contrastive training.\n",
        "Make sure to not add too many augmentations as that would change the fundamental structure of the input graph.\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"save\" : \"ssl_model\",\n",
        "    \"lr\" : 0.001,\n",
        "    \"epochs\" : 20,\n",
        "    \"batch_size\" : 64,\n",
        "    \"num_workers\" : 2,\n",
        "    \"dataset\" : \"proteins\", # Choices are [\"proteins\", \"enzymes\", \"collab\", \n",
        "                            # \"reddit_binary\", \"reddit_multi\", \"imdb_binary\", \n",
        "                            # \"imdb_multi\", \"dd\", \"mutag\", \"nci1\"]\n",
        "    \"model\" : \"gcn\", # choices are [\"gcn\", \"gin\", \"resgcn\", \"gat\", \"graphsage\", \"sgc\"]\n",
        "    \"feat_dim\" : 128,\n",
        "    \"layers\" : 3,\n",
        "    \"loss\" : \"infonce\", # choices are [\"infonce\", \"jensen_shannon\"]\n",
        "    \"augment_list\" : [\"edge_perturbation\", \"node_dropping\"],\n",
        "    # choices are [\"edge_perturbation\", \"diffusion\", \"diffusion_with_sample\", \n",
        "    # \"node_dropping\", \"random_walk_subgraph\", \"node_attr_mask\"]\n",
        "    \"train_data_percent\" : 1.0,\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttributeDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttributeDict(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkr-KBK4SQ4"
      },
      "source": [
        "## Loading the dataset and creating dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell deals with loading the data and splitting it into train, val and test splits. Further, we call our own custom dataloader which returns paired data -- the original graph and the positively augmented graph after applying the augmentations mentioned in the augment_list"
      ],
      "metadata": {
        "id": "fv8snPY110Bp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrfX2DlA0Lbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900a91ae-dbd8-4d56-e266-419a214a236d"
      },
      "source": [
        "from data import *\n",
        "\n",
        "dataset, input_dim, num_classes = load_dataset(args.dataset)\n",
        "\n",
        "# split the data into train / val / test sets\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "# build_loader is a dataloader which gives a paired sampled - the original x and the positively \n",
        "# augmented x obtained by applying the transformations in the augment_list as an argument\n",
        "train_loader = build_loader(args, train_dataset, \"train\")\n",
        "val_loader = build_loader(args, val_dataset, \"val\")\n",
        "test_loader = build_loader(args, test_dataset, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Extracting /tmp/TUDataset/PROTEINS/PROTEINS/PROTEINS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# samples in train subset: 779\n",
            "# samples in val subset: 222\n",
            "# samples in test subset: 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUgMpvfrDQPu"
      },
      "source": [
        "## Initializing the model and optimizer\n",
        "Here, the model comprises of only the GNN encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VCH2YFJ0rZd"
      },
      "source": [
        "from model import *\n",
        "\n",
        "# easy initialization of the GNN model encoder to map graphs to embeddings needed for contrastive training \n",
        "model = Encoder(input_dim, args.feat_dim, n_layers=args.layers, gnn=args.model)\n",
        "model = model.to(args.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeegrjUA8ro8"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block deals with training a self-supervised encoder using contrastive methods to get embeddings from raw graph data. This part of the code does not require the training samples to be labelled."
      ],
      "metadata": {
        "id": "VzYNYEPuMYU2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGCnmTLYOMSa"
      },
      "source": [
        "from loss import *\n",
        "\n",
        "def run(epoch, mode, dataloader):\n",
        "\tif mode == \"train\":\n",
        "\t\tmodel.train()\n",
        "\telif mode == \"val\" or mode == \"test\":\n",
        "\t\tmodel.eval()\n",
        "\n",
        "\tcontrastive_fn = eval(args.loss + \"()\")\n",
        "\n",
        "\tlosses = []\n",
        "\tfor data in dataloader:\n",
        "\t\tdata.to(args.device)\n",
        "\t\n",
        "\t\t# readout_anchor is the embedding of the original datapoint x on passing through the model\n",
        "\t\treadout_anchor = model((data.x_anchor, \n",
        "\t\t\t\t\t\t\t\tdata.edge_index_anchor, data.x_anchor_batch))\n",
        "\t\n",
        "\t\t# readout_positive is the embedding of the positively augmented x on passing through the model\n",
        "\t\treadout_positive = model((data.x_pos, \n",
        "\t\t\t\t\t\t\t\t\tdata.edge_index_pos, data.x_pos_batch))\n",
        "\n",
        "\t\t# negative samples for calculating the contrastive loss is computed in contrastive_fn\n",
        "\t\tloss = contrastive_fn(readout_anchor, readout_positive)\n",
        "\n",
        "\t\tif mode == \"train\":\n",
        "\t\t\t# backprop\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\n",
        "\t\t# keep track of loss values\n",
        "\t\tlosses.append(loss.item())\n",
        "\n",
        "\t# gather the results for the epoch\n",
        "\tepoch_loss = sum(losses) / len(losses)\n",
        "\treturn epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU9MzrDw3g6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6046e4de-b675-4e93-f501-a50d1b38545b"
      },
      "source": [
        "if not os.path.isdir(os.path.join(\"logs\", args.save)):\n",
        "    os.makedirs(os.path.join(\"logs\", args.save))\n",
        "\n",
        "best_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss = run(epoch, \"train\", train_loader)\n",
        "    val_loss = run(epoch, \"val\", val_loader)\n",
        "    log = \"Epoch {}, Train Loss: {:.3f}, Val Loss: {:.3f}\"\n",
        "    print(log.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    # save model\n",
        "    is_best_loss = False\n",
        "    if val_loss < best_val_loss:\n",
        "        best_epoch, best_train_loss, best_val_loss, is_best_loss = \\\n",
        "                                            epoch, train_loss, val_loss, True\n",
        "\n",
        "    model.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, \n",
        "                          best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "print(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "print(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 2.382, Val Loss: 3.534\n",
            "Epoch 1, Train Loss: 2.335, Val Loss: 2.816\n",
            "Epoch 2, Train Loss: 2.309, Val Loss: 2.611\n",
            "Epoch 3, Train Loss: 2.306, Val Loss: 2.679\n",
            "Epoch 4, Train Loss: 2.291, Val Loss: 2.421\n",
            "Epoch 5, Train Loss: 2.274, Val Loss: 2.604\n",
            "Epoch 6, Train Loss: 2.276, Val Loss: 2.409\n",
            "Epoch 7, Train Loss: 2.260, Val Loss: 2.449\n",
            "Epoch 8, Train Loss: 2.258, Val Loss: 2.430\n",
            "Epoch 9, Train Loss: 2.262, Val Loss: 2.427\n",
            "Epoch 10, Train Loss: 2.248, Val Loss: 2.400\n",
            "Epoch 11, Train Loss: 2.252, Val Loss: 2.324\n",
            "Epoch 12, Train Loss: 2.244, Val Loss: 2.442\n",
            "Epoch 13, Train Loss: 2.243, Val Loss: 2.368\n",
            "Epoch 14, Train Loss: 2.247, Val Loss: 2.393\n",
            "Epoch 15, Train Loss: 2.252, Val Loss: 2.899\n",
            "Epoch 16, Train Loss: 2.238, Val Loss: 2.420\n",
            "Epoch 17, Train Loss: 2.238, Val Loss: 2.612\n",
            "Epoch 18, Train Loss: 2.232, Val Loss: 2.644\n",
            "Epoch 19, Train Loss: 2.225, Val Loss: 2.767\n",
            "Train Loss at epoch 11 (best model): 2.252\n",
            "Val Loss at epoch 11 (best model): 2.324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liZ0kcN58kOA"
      },
      "source": [
        "## Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlplfD7g3G_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada96975-68c4-4908-fb9b-a949209e878c"
      },
      "source": [
        "best_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "model.eval()\n",
        "\n",
        "test_loss = run(best_epoch, \"test\", test_loader)\n",
        "print(\"Test Loss at epoch {}: {:.3f}\".format(best_epoch, test_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss at epoch 11: 2.334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11kpfAxY-pTw"
      },
      "source": [
        "# **Application on Downstream Task**\n",
        "In this section of the Colab, we will use the pretrained embeddings obtained from self-supervised model and train only the final few layers for the end goal of performing graph classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMOwoM-CEcH8"
      },
      "source": [
        "## Setting up arguments to train the classifier head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPSRBdBaEdOQ"
      },
      "source": [
        "'''\n",
        "Change these arguments to change either the dataset / model / train data percent\n",
        "train_data_percent is the fraction of training data which has labels associated. The utility of self-supervised \n",
        "training can be seen when train_data_percent is low and we can't train the entire model end-to-end.\n",
        "NOTE: The load argument will be the same as the save argument from the self-supervised training procedure\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"save\" : \"downstream_model\",\n",
        "    \"load\" : \"ssl_model\",\n",
        "    \"lr\" : 0.001,\n",
        "    \"epochs\" : 20,\n",
        "    \"batch_size\" : 64,\n",
        "    \"num_workers\" : 2,\n",
        "    \"dataset\" : \"proteins\", # Choices are [\"proteins\", \"enzymes\", \"collab\", \n",
        "                            # \"reddit_binary\", \"reddit_multi\", \"imdb_binary\", \n",
        "                            # \"imdb_multi\", \"dd\", \"mutag\", \"nci1\"]\n",
        "    \"model\" : \"gcn\", # choices are [\"gcn\", \"gin\", \"resgcn\", \"gat\", \"graphsage\", \"sgc\"]\n",
        "    \"feat_dim\" : 128,\n",
        "    \"layers\" : 3,\n",
        "    \"train_data_percent\" : 1.0,\n",
        "}\n",
        "\n",
        "args = AttributeDict(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZors8pMByhw"
      },
      "source": [
        "## Loading the dataset and creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP1f_OA7B0cm",
        "outputId": "8b11e2fd-c5f4-4e36-d649-c8f7fa27a543"
      },
      "source": [
        "dataset, input_dim, num_classes = load_dataset(args.dataset)\n",
        "\n",
        "# split the data into train / val / test sets\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "# build_classification_loader is a dataloader which gives one graph at a time\n",
        "train_loader = build_classification_loader(args, train_dataset, \"train\")\n",
        "val_loader = build_classification_loader(args, val_dataset, \"val\")\n",
        "test_loader = build_classification_loader(args, test_dataset, \"test\")\n",
        "\n",
        "print(\"Dataset split: {} {} {}\".format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "print(\"Number of classes: {}\".format(num_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split: 779 222 112\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3C1Zyt3Dz2H"
      },
      "source": [
        "## Initializing the model and optimizer\n",
        "Here, the model comprises of pretrained GNN encoder followed by classification layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szvJU7yvD1J7"
      },
      "source": [
        "# classification model is a GNN encoder followed by linear layer\n",
        "model = GraphClassificationModel(input_dim, args.feat_dim, n_layers=args.layers, output_dim=num_classes, gnn=args.model, load=args.load)\n",
        "model = model.to(args.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQL8ck4TCPtr"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-P_q7XADszX"
      },
      "source": [
        "def run(epoch, mode, dataloader):\n",
        "\tif mode == \"train\":\n",
        "\t\tmodel.train()\n",
        "\telif mode == \"val\" or mode == \"test\":\n",
        "\t\tmodel.eval()\n",
        "\n",
        "\t# CrossEntropy loss since it is a classification task\n",
        "\tloss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\tlosses = []\n",
        "\tcorrect = 0\n",
        "\tfor data in dataloader:\n",
        "\t\tdata.to(args.device)\n",
        "\n",
        "\t\tdata_input = data.x, data.edge_index, data.batch\n",
        "\t\tlabels = data.y\n",
        "\n",
        "\t\t# get class scores from model\n",
        "\t\tscores = model(data_input)\n",
        "\n",
        "\t\t# compute cross entropy loss\n",
        "\t\tloss = loss_fn(scores, labels)\n",
        "\n",
        "\t\tif mode == \"train\":\n",
        "\t\t\t# backprop\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\n",
        "\t\t# Keep track of loss and accuracy\n",
        "\t\tpred = scores.argmax(dim=1)\n",
        "\t\tcorrect += int((pred == labels).sum())\n",
        "\t\tlosses.append(loss.item())\n",
        "\n",
        "\t# gather the results for the epoch\n",
        "\tepoch_loss = sum(losses) / len(losses)\n",
        "\taccuracy = correct / len(dataloader.dataset)\n",
        "\treturn epoch_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoxbsVUpCSqB",
        "outputId": "d789333e-ab35-471d-9109-0faaec95f4fe"
      },
      "source": [
        "if not os.path.isdir(os.path.join(\"logs\", args.save)):\n",
        "    os.makedirs(os.path.join(\"logs\", args.save))\n",
        "\n",
        "best_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss, train_acc = run(epoch, \"train\", train_loader)\n",
        "    val_loss, val_acc = run(epoch, \"val\", val_loader)\n",
        "    log = \"Epoch {}, Train Loss: {:.3f}, Train Accuracy: {:.3f}, Val Loss: {:.3f}, Val Accuracy: {:.3f}\"\n",
        "    print(log.format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "    # save model\n",
        "    is_best_loss = False\n",
        "    if val_loss < best_val_loss:\n",
        "        best_epoch, best_train_loss, best_val_loss, is_best_loss = epoch, train_loss, val_loss, True\n",
        "\n",
        "    model.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "print(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "print(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 3.884, Train Accuracy: 0.540, Val Loss: 1.971, Val Accuracy: 0.640\n",
            "Epoch 1, Train Loss: 2.369, Train Accuracy: 0.629, Val Loss: 1.478, Val Accuracy: 0.622\n",
            "Epoch 2, Train Loss: 1.658, Train Accuracy: 0.614, Val Loss: 1.615, Val Accuracy: 0.644\n",
            "Epoch 3, Train Loss: 1.422, Train Accuracy: 0.660, Val Loss: 1.500, Val Accuracy: 0.640\n",
            "Epoch 4, Train Loss: 1.217, Train Accuracy: 0.635, Val Loss: 1.501, Val Accuracy: 0.631\n",
            "Epoch 5, Train Loss: 1.138, Train Accuracy: 0.632, Val Loss: 1.237, Val Accuracy: 0.667\n",
            "Epoch 6, Train Loss: 1.085, Train Accuracy: 0.655, Val Loss: 0.950, Val Accuracy: 0.667\n",
            "Epoch 7, Train Loss: 0.979, Train Accuracy: 0.668, Val Loss: 1.021, Val Accuracy: 0.667\n",
            "Epoch 8, Train Loss: 0.988, Train Accuracy: 0.682, Val Loss: 1.285, Val Accuracy: 0.667\n",
            "Epoch 9, Train Loss: 1.021, Train Accuracy: 0.661, Val Loss: 1.221, Val Accuracy: 0.658\n",
            "Epoch 10, Train Loss: 1.058, Train Accuracy: 0.687, Val Loss: 1.295, Val Accuracy: 0.644\n",
            "Epoch 11, Train Loss: 1.198, Train Accuracy: 0.653, Val Loss: 1.446, Val Accuracy: 0.649\n",
            "Epoch 12, Train Loss: 0.990, Train Accuracy: 0.670, Val Loss: 1.118, Val Accuracy: 0.640\n",
            "Epoch 13, Train Loss: 0.953, Train Accuracy: 0.683, Val Loss: 1.149, Val Accuracy: 0.644\n",
            "Epoch 14, Train Loss: 0.914, Train Accuracy: 0.688, Val Loss: 1.128, Val Accuracy: 0.640\n",
            "Epoch 15, Train Loss: 0.927, Train Accuracy: 0.715, Val Loss: 1.244, Val Accuracy: 0.644\n",
            "Epoch 16, Train Loss: 0.900, Train Accuracy: 0.703, Val Loss: 1.014, Val Accuracy: 0.662\n",
            "Epoch 17, Train Loss: 0.708, Train Accuracy: 0.709, Val Loss: 1.120, Val Accuracy: 0.662\n",
            "Epoch 18, Train Loss: 0.752, Train Accuracy: 0.689, Val Loss: 1.065, Val Accuracy: 0.626\n",
            "Epoch 19, Train Loss: 0.752, Train Accuracy: 0.692, Val Loss: 1.003, Val Accuracy: 0.662\n",
            "Train Loss at epoch 6 (best model): 1.085\n",
            "Val Loss at epoch 6 (best model): 0.950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model testing"
      ],
      "metadata": {
        "id": "76IiiiDYtr-R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7OXEWtsBjFJ",
        "outputId": "ba8b9d85-49ff-4010-bc6c-0b70ef6c91bc"
      },
      "source": [
        "best_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "model.eval()\n",
        "\n",
        "test_loss, test_accuracy = run(best_epoch, \"test\", test_loader)\n",
        "print(\"Test Loss at epoch {}: {:.3f}, Test Accuracy: {:.3f}\".format(best_epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss at epoch 6: 0.838, Test Accuracy: 0.696\n"
          ]
        }
      ]
    }
  ]
}